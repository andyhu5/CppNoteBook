# Bash 脚本：性能优化技巧



------

## 🚀 性能思维：先思考后编码

在深入研究具体技术之前，请先了解 Bash 性能优化是关于**减少系统调用**、**最小化子进程创建**以及**利用内置功能**。

**黄金法则：**每次调用外部命令都会产生开销。目标是用更少的外部调用完成更多的工作。

------

## ⚡ 1. 内置字符串操作 vs 外部命令

**缓慢的方法：**

```
# Don't do this - calls external commands repeatedly
for file in *.txt; do
    basename=$(basename "$file" .txt)
    dirname=$(dirname "$file")
    extension=$(echo "$file" | cut -d. -f2)
done
```



**快速方法：**

```
# Use parameter expansion instead
for file in *.txt; do
    basename="${file##*/}"      # Remove path
    basename="${basename%.*}"   # Remove extension
    dirname="${file%/*}"        # Extract directory
    extension="${file##*.}"     # Extract extension
done
```



**性能影响：**大型文件列表的速度最高可提高 10 倍。

------

## 🔄 2. 高效的数组处理

**缓慢的方法：**

```
# Inefficient - recreates array each time
users=()
while IFS= read -r user; do
    users=("${users[@]}" "$user")  # This gets slower with each iteration
done < users.txt
```



**快速方法：**

```
# Efficient - use mapfile for bulk operations
mapfile -t users < users.txt

# Or for processing while reading
while IFS= read -r user; do
    users+=("$user")  # Much faster than recreating array
done < users.txt
```



**为什么它更快：** `+=`高效追加，同时`("${users[@]}" "$user")`重新创建整个数组。

------

## 📁 3. 智能文件处理模式

**缓慢的方法：**

```
# Reading file multiple times
line_count=$(wc -l < large_file.txt)
word_count=$(wc -w < large_file.txt)
char_count=$(wc -c < large_file.txt)
```



**快速方法：**

```
# Single pass through file
read_stats() {
    local file="$1"
    local lines=0 words=0 chars=0

    while IFS= read -r line; do
        ((lines++))
        words+=$(echo "$line" | wc -w)
        chars+=${#line}
    done < "$file"

    echo "Lines: $lines, Words: $words, Characters: $chars"
}
```



**甚至更好 - 尽可能使用内置：**

```
# Let the system do what it's optimized for
stats=$(wc -lwc < large_file.txt)
echo "Stats: $stats"
```



------

## 🎯 4. 条件逻辑优化

**缓慢的方法：**

```
# Multiple separate checks
if [[ -f "$file" ]]; then
    if [[ -r "$file" ]]; then
        if [[ -s "$file" ]]; then
            process_file "$file"
        fi
    fi
fi
```



**快速方法：**

```
# Combined conditions
if [[ -f "$file" && -r "$file" && -s "$file" ]]; then
    process_file "$file"
fi

# Or use short-circuit logic
[[ -f "$file" && -r "$file" && -s "$file" ]] && process_file "$file"
```



------

## 🔍 5. 模式匹配性能

**缓慢的方法：**

```
# External grep for simple patterns
if echo "$string" | grep -q "pattern"; then
    echo "Found pattern"
fi
```



**快速方法：**

```
# Built-in pattern matching
if [[ "$string" == *"pattern"* ]]; then
    echo "Found pattern"
fi

# Or regex matching
if [[ "$string" =~ pattern ]]; then
    echo "Found pattern"
fi
```



**性能比较：**对于简单模式，内置匹配比外部 grep 快 5-20 倍。

------

## 🏃 6. 循环优化策略

**缓慢的方法：**

```
# Inefficient command substitution in loop
for i in {1..1000}; do
    timestamp=$(date +%s)
    echo "Processing item $i at $timestamp"
done
```



**快速方法：**

```
# Move expensive operations outside loop when possible
start_time=$(date +%s)
for i in {1..1000}; do
    echo "Processing item $i at $start_time"
done

# Or batch operations
{
    for i in {1..1000}; do
        echo "Processing item $i"
    done
} | while IFS= read -r line; do
    echo "$line at $(date +%s)"
done
```



------

## 💾 7. 内存高效的数据处理

**缓慢的方法：**

```
# Loading entire file into memory
data=$(cat huge_file.txt)
process_data "$data"
```



**快速方法：**

```
# Stream processing
process_file_stream() {
    local file="$1"
    while IFS= read -r line; do
        # Process line by line
        process_line "$line"
    done < "$file"
}
```



**对于大型数据集：**

```
# Use temporary files for intermediate processing
mktemp_cleanup() {
    local temp_files=("$@")
    rm -f "${temp_files[@]}"
}

process_large_dataset() {
    local input_file="$1"
    local temp1 temp2
    temp1=$(mktemp)
    temp2=$(mktemp)

    # Clean up automatically
    trap "mktemp_cleanup '$temp1' '$temp2'" EXIT

    # Multi-stage processing with temporary files
    grep "pattern1" "$input_file" > "$temp1"
    sort "$temp1" > "$temp2"
    uniq "$temp2"
}
```



------

## 🚀 8. 正确进行并行处理

**基本并行模式：**

```
# Process multiple items in parallel
parallel_process() {
    local items=("$@")
    local max_jobs=4
    local running_jobs=0
    local pids=()

    for item in "${items[@]}"; do
        # Launch background job
        process_item "$item" &
        pids+=($!)
        ((running_jobs++))

        # Wait if we hit max concurrent jobs
        if ((running_jobs >= max_jobs)); then
            wait "${pids[0]}"
            pids=("${pids[@]:1}")  # Remove first PID
            ((running_jobs--))
        fi
    done

    # Wait for remaining jobs
    for pid in "${pids[@]}"; do
        wait "$pid"
    done
}
```



**高级：作业队列模式：**

```
# Create a job queue for better control
create_job_queue() {
    local queue_file
    queue_file=$(mktemp)
    echo "$queue_file"
}

add_job() {
    local queue_file="$1"
    local job_command="$2"
    echo "$job_command" >> "$queue_file"
}

process_queue() {
    local queue_file="$1"
    local max_parallel="${2:-4}"

    # Use xargs for controlled parallel execution
    cat "$queue_file" | xargs -n1 -P"$max_parallel" -I{} bash -c '{}'
    rm -f "$queue_file"
}
```



------

## 📊 9. 性能监控和分析

**内置计时：**

```
# Time specific operations
time_operation() {
    local operation_name="$1"
    shift

    local start_time
    start_time=$(date +%s.%N)

    "$@"  # Execute the operation

    local end_time
    end_time=$(date +%s.%N)
    local duration
    duration=$(echo "$end_time - $start_time" | bc)

    echo "Operation '$operation_name' took ${duration}s" >&2
}

# Usage
time_operation "file_processing" process_large_file data.txt
```



**资源使用情况监控：**

```
# Monitor script resource usage
monitor_resources() {
    local script_name="$1"
    shift

    # Start monitoring in background
    {
        while kill -0 $$ 2>/dev/null; do
            ps -o pid,pcpu,pmem,etime -p $$
            sleep 5
        done
    } > "${script_name}_resources.log" &
    local monitor_pid=$!

    # Run the actual script
    "$@"

    # Stop monitoring
    kill "$monitor_pid" 2>/dev/null || true
}
```



------

## 🔧 10. 真实世界优化示例

以下是显示优化前/后的完整示例：

**之前（慢速版）：**

```
#!/bin/bash
# Processes log files - SLOW version

process_logs() {
    local log_dir="$1"
    local results=()

    for log_file in "$log_dir"/*.log; do
        # Multiple file reads
        error_count=$(grep -c "ERROR" "$log_file")
        warn_count=$(grep -c "WARN" "$log_file")
        total_lines=$(wc -l < "$log_file")

        # Inefficient string building
        result="File: $(basename "$log_file"), Errors: $error_count, Warnings: $warn_count, Lines: $total_lines"
        results=("${results[@]}" "$result")
    done

    # Process results
    for result in "${results[@]}"; do
        echo "$result"
    done
}
```



**之后（优化版本）：**

```
#!/bin/bash
# Processes log files - OPTIMIZED version

process_logs_fast() {
    local log_dir="$1"
    local temp_file
    temp_file=$(mktemp)

    # Process all files in parallel
    find "$log_dir" -name "*.log" -print0 | \
    xargs -0 -n1 -P4 -I{} bash -c '
        file="{}"
        basename="${file##*/}"

        # Single pass through file
        errors=0 warnings=0 lines=0
        while IFS= read -r line || [[ -n "$line" ]]; do
            ((lines++))
            [[ "$line" == *"ERROR"* ]] && ((errors++))
            [[ "$line" == *"WARN"* ]] && ((warnings++))
        done < "$file"

        printf "File: %s, Errors: %d, Warnings: %d, Lines: %d\n" \
            "$basename" "$errors" "$warnings" "$lines"
    ' > "$temp_file"

    # Output results
    sort "$temp_file"
    rm -f "$temp_file"
}
```



**性能改进：**典型日志目录的速度提高 70%。

------

## 💡 性能最佳实践总结

1. 尽可能**使用内置操作而不是外部命令**
2. **尽量减少子流程创建**- 尽可能进行批量操作
3. **使用流数据**而不是将所有内容加载到内存中
4. **利用并行处理**来处理 CPU 密集型任务
5. **分析脚本**以确定实际的瓶颈
6. **使用适当的数据结构**- 列表使用数组，查找使用关联数组
7. **优化循环**- 尽可能将昂贵的操作移到外部
8. **高效处理大文件**- 逐行处理，使用临时文件